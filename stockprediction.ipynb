{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMTZJPh1R7nX3IzuVtTdSMj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayush6233/Sentiment-aware-LSTM-for-confident-time-series-forecasting./blob/main/stockprediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0UNjC-hR25s",
        "outputId": "de913cd6-ebdc-4923-d5ac-65520c1c6d25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2898926691.py:18: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  data = yf.download(ticker, start=start_date, end=end_date, progress=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2898926691.py:37: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
            "  dt = datetime.utcfromtimestamp(int(ts)).date()\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 177ms/step - loss: 0.1356 - mae: 0.3492 - val_loss: 0.0965 - val_mae: 0.2991\n",
            "Epoch 2/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - loss: 0.0430 - mae: 0.1829 - val_loss: 0.0313 - val_mae: 0.1570\n",
            "Epoch 3/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0122 - mae: 0.0877 - val_loss: 0.0082 - val_mae: 0.0697\n",
            "Epoch 4/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - loss: 0.0183 - mae: 0.1148 - val_loss: 0.0065 - val_mae: 0.0700\n",
            "Epoch 5/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.0063 - mae: 0.0632 - val_loss: 0.0294 - val_mae: 0.1513\n",
            "Epoch 6/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.0111 - mae: 0.0863 - val_loss: 0.0259 - val_mae: 0.1396\n",
            "Epoch 7/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.0088 - mae: 0.0740 - val_loss: 0.0094 - val_mae: 0.0886\n",
            "Epoch 8/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.0069 - mae: 0.0667 - val_loss: 0.0083 - val_mae: 0.0834\n",
            "Epoch 9/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 0.0054 - mae: 0.0616 - val_loss: 0.0153 - val_mae: 0.1081\n",
            "Epoch 10/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 214ms/step - loss: 0.0068 - mae: 0.0674 - val_loss: 0.0181 - val_mae: 0.1156\n",
            "Epoch 11/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 226ms/step - loss: 0.0075 - mae: 0.0705 - val_loss: 0.0128 - val_mae: 0.1011\n",
            "Epoch 12/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - loss: 0.0066 - mae: 0.0663 - val_loss: 0.0108 - val_mae: 0.0941\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 316ms/step\n",
            "MAE = 8.3435\n",
            "MAPE = 3.6332%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import os, requests\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from datetime import datetime\n",
        "ticker = \"AAPL\"\n",
        "start_date = \"2024-04-01\"\n",
        "end_date = \"2025-04-01\"\n",
        "window_size = 60\n",
        "\n",
        "## Fetch prices\n",
        "data = yf.download(ticker, start=start_date, end=end_date, progress=False)\n",
        "prices = data['Close'].copy()\n",
        "dates = prices.index\n",
        "N = len(prices)\n",
        "## we will ofc assume N>=60\n",
        "\n",
        "## fetch news\n",
        "FINNHUB_KEY = os.getenv(\"FINNHUB_API_KEY\", \"d2tei5hr01qr5a729ho0d2tei5hr01qr5a729hog\").strip()\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "def fetch_finnhub_news(symbol, from_date, to_date, api_key):\n",
        "    url = \"https://finnhub.io/api/v1/company-news\"\n",
        "    params = {\"symbol\": symbol, \"from\": from_date, \"to\": to_date, \"token\": api_key}\n",
        "    r = requests.get(url, params=params, timeout=20)\n",
        "    r.raise_for_status()\n",
        "    items = r.json()\n",
        "    rows = []\n",
        "    for it in items:\n",
        "        ts = it.get(\"datetime\")\n",
        "        dt = datetime.utcfromtimestamp(int(ts)).date()\n",
        "        text = (it.get(\"headline\",\"\") + \". \" + (it.get(\"summary\") or \"\"))\n",
        "        rows.append({\"date\": pd.to_datetime(dt), \"text\": text, \"url\": it.get(\"url\")})\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "df_news = fetch_finnhub_news(ticker, start_date, end_date, FINNHUB_KEY)\n",
        "\n",
        "# If empty, create zero-sentiment series for all trading days\n",
        "\n",
        "# compute VADER scores and average per day\n",
        "df_news['compound'] = df_news['text'].apply(lambda t: analyzer.polarity_scores(str(t))['compound'])\n",
        "daily_sent = df_news.groupby('date')['compound'].mean()\n",
        "\n",
        "daily_sent_full = daily_sent.reindex(dates, fill_value=0.0)\n",
        "\n",
        "price_values = prices.values.reshape(-1, 1)\n",
        "sentiment_values = daily_sent_full.values.reshape(-1, 1)\n",
        "\n",
        "\n",
        "S = N - window_size\n",
        "train_samples = int(S * 0.8)\n",
        "test_samples = S - train_samples\n",
        "\n",
        "train_raw_end_idx = window_size + train_samples - 1\n",
        "train_raw_end_idx_plus1 = train_raw_end_idx + 1\n",
        "\n",
        "\n",
        "price_scaler = MinMaxScaler()\n",
        "sentiment_scaler = MinMaxScaler()\n",
        "price_scaler.fit(price_values[:train_raw_end_idx_plus1])\n",
        "sentiment_scaler.fit(sentiment_values[:train_raw_end_idx_plus1])\n",
        "\n",
        "scaled_prices = price_scaler.transform(price_values)\n",
        "scaled_sentiments = sentiment_scaler.transform(sentiment_values)\n",
        "\n",
        "# Build equences\n",
        "X, y = [], []\n",
        "for i in range(window_size, N):\n",
        "    price_win = scaled_prices[i-window_size:i, 0]\n",
        "    sent_win = scaled_sentiments[i-window_size:i, 0]\n",
        "    X.append(np.column_stack((price_win, sent_win)))\n",
        "    y.append(scaled_prices[i, 0])\n",
        "X = np.array(X)           # shape (S, window_size, 2)\n",
        "y = np.array(y).reshape(-1, 1)\n",
        "\n",
        "\n",
        "X_train, X_test = X[:train_samples], X[train_samples:]\n",
        "y_train, y_test = y[:train_samples], y[train_samples:]\n",
        "\n",
        "# # quick sanity checks\n",
        "# assert X_train.shape[0] == y_train.shape[0]\n",
        "# assert X_test.shape[0] == y_test.shape[0]\n",
        "\n",
        "# model\n",
        "model = Sequential([\n",
        "    LSTM(64, return_sequences=True, input_shape=(window_size, 2)),\n",
        "    Dropout(0.2),\n",
        "    LSTM(32),\n",
        "    Dense(1)\n",
        "])\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "es = EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True)\n",
        "\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.1, callbacks=[es], shuffle=False, verbose=1)\n",
        "\n",
        "# Predict and inverse transform\n",
        "pred_scaled = model.predict(X_test)\n",
        "true_scaled = y_test\n",
        "\n",
        "pred_prices = price_scaler.inverse_transform(pred_scaled)\n",
        "true_prices = price_scaler.inverse_transform(true_scaled)\n",
        "\n",
        "#evaluation\n",
        "mae = mean_absolute_error(true_prices, pred_prices)\n",
        "eps = 1e-8\n",
        "mape = np.mean(np.abs((true_prices - pred_prices) / np.maximum(np.abs(true_prices), eps))) * 100\n",
        "print(f\"MAE = {mae:.4f}\")\n",
        "print(f\"MAPE = {mape:.4f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZpAjt2H1SBqB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}